{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('d2l': conda)"
  },
  "interpreter": {
   "hash": "b5471a7441e71a2e9077fc9f1edb69a69142e0d51daa994c89e6186fa4151e97"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Go ahead\n"
     ]
    }
   ],
   "source": [
    "%config IPCompleter.greedy = True\n",
    "#   Above is for the IntelliSense module to work.\n",
    "import tensorflow as tf\n",
    "#   To prevent a total meltdown of my GPU, restrictions needed to be made.\n",
    "rtx2080 = tf.config.experimental.list_physical_devices('GPU')\n",
    "for num in rtx2080:\n",
    "    tf.config.experimental.set_memory_growth(num, True)\n",
    "print('Go ahead')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tensorflow\n",
    "from d2l import tensorflow as d2l\n",
    "\n",
    "true_w = tf.constant([2,-3.4])\n",
    "true_b = 4.2\n",
    "features, labels = d2l.synthetic_data(true_w, true_b, 1000)\n"
   ]
  },
  {
   "source": [
    "As we can see, in this book - we'll be doing the same damn thing just as the earlier one, \n",
    "except that we shall call the higher council magic of KERAS, the TensorFlow High-Level API for DL/ML."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "def load_array(data_arrays, batch_size, is_train = True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(data_arrays)\n",
    "    if is_train:\n",
    "        dataset = dataset.shuffle(buffer_size=1000)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "batch_size = 10\n",
    "data_iter = load_array((features, labels), batch_size)\n",
    "\n",
    "next(iter(data_iter))"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(10, 2), dtype=float32, numpy=\n",
       " array([[-0.25415364,  0.452938  ],\n",
       "        [ 0.9286384 , -1.7490934 ],\n",
       "        [ 0.6678283 , -0.22204627],\n",
       "        [-0.7659453 ,  0.15152474],\n",
       "        [-0.06647165,  1.5990258 ],\n",
       "        [-0.15157346,  0.2951088 ],\n",
       "        [ 0.7398171 , -2.759268  ],\n",
       "        [-0.15879342,  0.26650652],\n",
       "        [ 1.453479  ,  0.5748983 ],\n",
       "        [ 0.05668436,  0.20966102]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
       " array([[ 2.14065  ],\n",
       "        [11.99943  ],\n",
       "        [ 6.284743 ],\n",
       "        [ 2.15122  ],\n",
       "        [-1.3549905],\n",
       "        [ 2.8910851],\n",
       "        [15.061181 ],\n",
       "        [ 2.9596896],\n",
       "        [ 5.1659794],\n",
       "        [ 3.6098025]], dtype=float32)>)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'trainable_varibles'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-3fdc5bf6201a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_varibles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"epoch {epoch + 1}, loss {1:f}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'trainable_varibles'"
     ]
    }
   ],
   "source": [
    "#   Now the high council magic shit:\n",
    "net = tf.keras.Sequential()\n",
    "net.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "initializer = tf.initializers.RandomNormal(stddev=0.01)\n",
    "net = tf.keras.Sequential()\n",
    "net.add(tf.keras.layers.Dense(1, kernel_initializer=initializer))\n",
    "\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "trainer = tf.keras.optimizers.SGD(learning_rate=0.03)\n",
    "\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter:\n",
    "        with tf.GradientTape() as tape:\n",
    "            l = loss(net(X, training=True),y)\n",
    "        grads = tape.gradient(l, net.trainable_variables)\n",
    "        trainer.apply_gradients(zip(grads, net.trainable_variables))\n",
    "    l = loss(net(features), labels)\n",
    "    print(f\"epoch {epoch + 1}, loss {1:f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = net.get_weights()[0]\n",
    "print(f\"Error in estimating w:\", true_w - tf.reshape(w, true_w.shape))\n",
    "b = net.get_weights()[1]\n",
    "print(f\"Error in estimating b:\", true_b - b)"
   ]
  }
 ]
}